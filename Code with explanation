---
title: "Segmentación de clientes"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

### Práctica de aprendizaje No Supervisado con K-Means
#### Álvaro Barrio Hernández (alvaro.barrio.hernandez@gmail.com)

___

Código: [GitHub](https://github.com/alvarobarrio)


La segmentación de clientes es una de las aplicaciones más importantes del aprendizaje no supervisado. Mediante técnicas de agrupación en clústeres, las empresas pueden identificar los distintos segmentos de clientes, lo que les permite dirigirse a la base de usuarios potenciales. En este proyecto de aprendizaje automático, utilizaremos la agrupación en clústeres de K-means, que es el algoritmo esencial para agrupar conjuntos de datos sin etiquetar. 

¿Qué es la segmentación de clientes?
La segmentación de clientes es el proceso de división del conjunto de clientes en varios grupos de personas que comparten características relevantes para el marketing, como el género, la edad, los intereses y los hábitos de consumo.

Las empresas que implementan la segmentación de clientes tienen la idea de que cada cliente tiene requisitos diferentes y requieren un esfuerzo de marketing específico para abordarlos de manera adecuada. Las empresas tienen como objetivo obtener un enfoque más profundo del cliente al que se dirigen. Por lo tanto, su objetivo debe ser específico y debe adaptarse a los requisitos de todos y cada uno de los clientes. Además, a través de los datos recopilados, las empresas pueden obtener una comprensión más profunda de las preferencias de los clientes, así como los requisitos para descubrir segmentos valiosos que les reportarían el máximo beneficio. De esta manera, pueden diseñar estrategias de sus técnicas de marketing de manera más eficiente y minimizar la posibilidad de riesgo para su inversión.

La técnica de segmentación de clientes depende de varios diferenciadores clave que dividen a los clientes en grupos a los que dirigirse. Los datos relacionados con la demografía, la geografía, la situación económica, así como los patrones de comportamiento, juegan un papel crucial en la determinación de la dirección de la empresa para abordar los distintos segmentos.
___
Comencemos por instalar los paquetes necesarios para la correcta realización del ejercicio:
```{r}

library(umap)
library(ggplot2)
library(plotrix)
library(purrr)
library(cluster) 
library(gridExtra)
library(grid)
library(NbClust)
library(factoextra)
library(tinytex)
library(tidyverse)
library(readr)
library(Rtsne)
```

### 1. Datos y características

Comenzamos por la carga de datos desde un archivo csv:
```{r}
customer_data=read.csv("Mall_Customers.csv")

```


Chequeamos la información básica del dataset:
```{r}
head(customer_data) 
names(customer_data)
str(customer_data)
summary(customer_data)

```


Observamos como la variable Age tiene su mínimo en 18 y máximo en 70.

El 'Anual Income' comienza en 15000 y termina en 137000.

El 'Spending Score' se distribuye en el intervalo 1 a 99.

___

### 2. Visualizaciones 

En este apartado se plantean diversas figuras para graficar el comportamiento de cada variable


```{r echo=FALSE}
ggplot(customer_data, aes(Gender)) +
  geom_bar(aes(fill = Gender)) + 
  theme_minimal() +
  ggtitle("División por Género") +
  xlab("Contador") + 
  ylab("Género") +
  geom_text(stat='count', aes(x = Gender, label = ..count..), vjust = -0.5)
```

El conjunto de datos contiene un número mayor de registros femeninos frente a masculinos



```{r echo=FALSE}
ggplot(data = customer_data, aes(Age)) +
  geom_histogram(aes(Age, fill = Gender), binwidth = 1, color = "blue") + 
  theme_minimal() +
  ggtitle("División por Edad") +
  xlab("Edad") + 
  ylab("Frecuencia") +
  geom_text(stat='count', aes(x = Age, label = ..count..), vjust = -1)

```

Se comprueba como la franja de 30 a 35 años contiene a un gran número de clientes
El más joven tiene 18 y el mayor 70



```{r echo=FALSE}
ggplot(data = customer_data, aes(Annual.Income..k..)) +
  geom_histogram(aes(Annual.Income..k.., fill = Gender), binwidth = 10, color = "blue") + 
  theme_minimal() +
  ggtitle("Ingresos Anuales") +
  xlab("Ingreso Anual") + 
  ylab("Frecuencia")
```


```{r echo=FALSE}
ggplot(customer_data) +
  geom_density(aes(Annual.Income..k.., fill = Gender)) +
  facet_grid(Gender~., scales = 'free') +
  xlim(-5, 170)
```

Ingreso mínimo es 15 y máximo 137. 
Se observa una distribución normal



```{r echo=FALSE}
#Boxplot
ggplot(customer_data, aes(Spending.Score..1.100.))+
  geom_boxplot(color = "blue", fill = "gold") +
  theme_minimal() +
  ggtitle("Boxplot para Spending Score") +
  xlab("Age") 

#Histograma
ggplot(customer_data, aes(Spending.Score..1.100.)) +
  geom_histogram(color = "blue", fill = "gold", bins = 10 ) +
  geom_text(stat='count', aes(x = Spending.Score..1.100., label = ..count..), vjust = -1)

#Scatter
ggplot(customer_data, aes(x =Annual.Income..k.., y = Spending.Score..1.100.)) + 
  geom_point(stat = "identity", aes(color = Gender))
```

El mínimo es 1, el máximo es 99 y la media es 50.

Los clientes enttre 400 y 50 años son los que obtienen mayor puntuación en gasto

___

### 3. Aplicación de Algoritmo de k-Means 


1. Para empezar, primero seleccionamos un número de clústeres para usar e inicializamos aleatoriamente sus respectivos puntos centrales. 
Para calcular el número de clústeres a utilizar, es bueno echar un vistazo rápido a los datos y tratar de identificar cualquier agrupación distinta. 
Los puntos centrales son vectores de la misma longitud que cada vector de puntos de datos.

2. Cada punto de datos se clasifica calculando la distancia entre ese punto y cada centro del clúster, y luego clasificando el punto que estará en el clúster cuyo centro está más cerca de él.

3. Basándonos en estos puntos clasificados, recalculamos el centro del clúster tomando la media de todos los vectores del clúster.
4. Repite estos pasos para un número determinado de iteraciones o hasta que los centros de clústeres no cambien mucho entre iteraciones. 
También puedes optar por inicializar aleatoriamente los centros de grupo unas cuantas veces, y luego seleccionar la ejecución que parezca que proporcionó los mejores resultados.


K Means tiene la ventaja de que es bastante rápido, ya que todo lo que estamos haciendo es calcular las distancias entre puntos y centros de grupo, por lo tanto, son muy pocos cálculos.


Debemos especificar el número de clústers a utilizar.
Utilizaremos 3 métodos para seleccionarlo.


#### Elbow Method

Probablemente el método más conocido, el método del codo, en el que se calcula y grafica la suma de cuadrado en cada número de clústeres, y allí buscas un cambio de pendiente de empinada a poca profundidad, un codo, para determinar el número óptimo de clústeres.
Este método es inexacto, pero sigue siendo potencialmente útil.

Este método funciona de la siguiente forma, se calcula la suma de errores cuadráticos dentro del clúster para diferentes valores de K y se elige la K para la cual la suma de errores cuadráticos comienza a disminuir.
Esto es visible como un codo:

```{r echo=FALSE}
set.seed(123)
#Funcion para calcular la suma de cuadrados total intra-cluster
  iss <- function(k) {
  kmeans(customer_data[,3:5],k,iter.max=100,nstart=100,algorithm="Lloyd" )$tot.withinss
}

k.values <- 1:10


iss_values <- map_dbl(k.values, iss)

plot(k.values, iss_values,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total intra-clusters sum of squares")

```

#### Método de la silueta

El método de la silueta puede utilizarse para estudiar la distancia de separación entre los grupos resultantes. Muestra una medida de cuán cerca está cada punto en un clúster de los puntos en los clústeres vecinos y por lo tanto proporciona una forma de evaluar parámetros como el número de clústeres visualmente. Este método es mejor ya que hace que la decisión sobre el número óptimo de clústeres sea más significativa y clara.

Pero esta métrica es costosa de calcular ya que el coeficiente se calcula para cada caso. Por lo tanto, la decisión sobre la métrica óptima a elegir para el número de clústeres se debe tomar de acuerdo con las necesidades del producto.

Esta medida tiene un rango de -1 a 1. Donde 1 significa que los puntos están muy cerca de su propio clúster y lejos de otros clústeres, mientras que -1 indica que los puntos están cerca de los clústeres vecinos:

```{r echo=FALSE}

k2<-kmeans(customer_data[,3:5],2,iter.max=100,nstart=50,algorithm="Lloyd")
s2<-plot(silhouette(k2$cluster,dist(customer_data[,3:5],"euclidean")))

k3<-kmeans(customer_data[,3:5],3,iter.max=100,nstart=50,algorithm="Lloyd")
s3<-plot(silhouette(k3$cluster,dist(customer_data[,3:5],"euclidean")))

k4<-kmeans(customer_data[,3:5],4,iter.max=100,nstart=50,algorithm="Lloyd")
s4<-plot(silhouette(k4$cluster,dist(customer_data[,3:5],"euclidean")))

k5<-kmeans(customer_data[,3:5],5,iter.max=100,nstart=50,algorithm="Lloyd")
s5<-plot(silhouette(k5$cluster,dist(customer_data[,3:5],"euclidean")))

k6<-kmeans(customer_data[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
s6<-plot(silhouette(k6$cluster,dist(customer_data[,3:5],"euclidean")))

k7<-kmeans(customer_data[,3:5],7,iter.max=100,nstart=50,algorithm="Lloyd")
s7<-plot(silhouette(k7$cluster,dist(customer_data[,3:5],"euclidean")))

k8<-kmeans(customer_data[,3:5],8,iter.max=100,nstart=50,algorithm="Lloyd")
s8<-plot(silhouette(k8$cluster,dist(customer_data[,3:5],"euclidean")))

k9<-kmeans(customer_data[,3:5],9,iter.max=100,nstart=50,algorithm="Lloyd")
s9<-plot(silhouette(k9$cluster,dist(customer_data[,3:5],"euclidean")))

k10<-kmeans(customer_data[,3:5],10,iter.max=100,nstart=50,algorithm="Lloyd")
s10<-plot(silhouette(k10$cluster,dist(customer_data[,3:5],"euclidean")))


#Visualizar el valor óptimo
fviz_nbclust(customer_data[,3:5], kmeans, method = "silhouette")

```

#### Método de estadística de brecha ####

El método de estadística de brecha compara el total dentro de la variación intraclúster para diferentes valores de K con sus valores esperados bajo una distribución de referencia nula de los datos. 
#La estimación de los clustering óptimos será un valor que maximice la estadística de la brecha, es decir, que produzca la estadística de la brecha más grande.Esto significa que la estructura de agrupación está muy lejos de la distribución uniforme y aleatoria de los puntos.

El gráfico de estadísticas de brecha muestra las estadísticas por número de clústeres con errores estándar dibujados con segmentos verticales y el valor óptimo de K marcado con una línea azul discontinua vertical.


```{r echo=FALSE}

set.seed(333)
stat_gap <- clusGap(customer_data[,3:5], FUN = kmeans, nstart = 25,
                    K.max = 10, B = 50)
fviz_gap_stat(stat_gap)

```

Elegimos k = 6 como número de cústers:
```{r echo=FALSE}
k6<-kmeans(customer_data[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
k6
```

___

### 4. Visualización de Clústers #### 
```{r echo=FALSE}
pcclust=prcomp(customer_data[,3:5],scale=FALSE) 
summary(pcclust)

pcclust$rotation[,1:2]

#visualize clusters
set.seed(333)

#Annual Income & Spending Score
ggplot(customer_data, aes(x =Annual.Income..k.., y = Spending.Score..1.100.)) + 
  geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
  scale_color_discrete(name=" ",
                       breaks=c("1", "2", "3", "4", "5","6"),
                       labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Mall Customers", subtitle = "K-means Clustering")+
  xlab("Ingreso") + 
  ylab("Gasto")



```

Podemos observar una clasificación de cluster de la siguiente forma:

* Clúster 1 y 2 – Ingreso medio y gasto medio

* Clúster 3 – Ingreso alto y gasto alto

* Clúster 4 – Ingreso bajo y gasto bajo

* Clúster 5 – Ingreso bajo y gasto alto

* Clúster 6 – Ingreso alto y gasto bajo

```{r echo=FALSE}

#Spending Score & Age
ggplot(customer_data, aes(x =Spending.Score..1.100., y =Age)) + 
  geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
  scale_color_discrete(name=" ",
                       breaks=c("1", "2", "3", "4", "5","6"),
                       labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Mall Customers") + 
  xlab("Gasto") + 
  ylab("Edad")
```


Podemos observar una clasificación de cluster de la siguiente forma:

* Clúster 1 - Gasto medio y edad alta

* Clúster 2 – Gasto medio y edad baja

* Clúster 3 y 5 – Gasto alto y edad baja

* Clúster 4 y 6 – Gasto bajo 

___

#### 5. Principal Components Analysis (PCA) & t-distributed stochastic neighbor embedding (t-sne)
```{r echo=FALSE}

kCols=function(vec){cols=rainbow (length (unique (vec)))
return (cols[as.numeric(as.factor(vec))])}

digCluster<-k6$cluster; dignm<-as.character(digCluster); # K-means clusters

plot(pcclust$x[,1:2], col =kCols(digCluster),pch =19,xlab ="PCA-1",ylab="PCA-2", main="PCA")
legend("bottomleft",unique(dignm),fill=unique(kCols(digCluster)))

```
Distribución:

* Clúster 4 y 1: estos dos clústeres están formados por clientes con puntuación PCA1 media y PCA2 media.

* Clúster 6: este clúster representa a los clientes que tienen un PCA2 alto y un PCA1 bajo.

* Clúster 5: en este grupo, hay clientes con una puntuación de PCA1 media y una puntuación de PCA2 baja.

* Clúster 3: este clúster se compone de clientes con un alto ingreso de PCA1 y un alto PCA2.

* Clúster 2: se compone de clientes con un PCA2 alto y un gasto anual medio de ingresos.


```{r echo=FALSE}
#TSNE
tsne <- Rtsne(customer_data[,3:5], dims = 2, perplexity=30, verbose=TRUE, max_iter = 500)
summary(tsne)
exeTimeTsne<- system.time(Rtsne(customer_data[,3:5], dims = 2, perplexity=30, verbose=TRUE, max_iter = 500))
## Plotting
plot(tsne$Y, t='n', main="tsne")
text(tsne$Y, xlab ="K-means", ylab="classes", col=kCols(digCluster))

```

___

Con la ayuda de la agrupación en clústeres, podemos comprender mucho mejor las variables, lo que nos impulsa a tomar decisiones cuidadosas. Con la identificación de clientes, las empresas pueden lanzar productos y servicios que se dirigen a los clientes en función de varios parámetros como ingresos, edad, patrones de gasto, etc. Además, se toman en consideración patrones más complejos como revisiones de productos para una mejor segmentación.
 
En este proyecto de ciencia de datos, pasamos por el modelo de segmentación de clientes. Desarrollamos esto utilizando aprendizaje no supervisado. Específicamente, hicimos uso de un algoritmo de agrupamiento llamado agrupamiento de K-medias. Analizamos y visualizamos los datos y luego procedimos a implementar nuestro algoritmo. Pueden encontrar todo el código disonible en mi [GitHub](https://github.com/alvarobarrio)



